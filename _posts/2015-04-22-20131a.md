---
layout: post
title: Jam 2013 Round 1A
---

As ever, links to: [Official Contest Analysis](https://code.google.com/codejam/contest/2418487/dashboard#s=a) and my code on [GitHub](https://github.com/MatthewDaws/CodeJam/tree/master/2013_1a).

I did this under timed conditions: a mild disaster, but then it was for everyone back in 2013.  If I had been clinical, I could have solved A, B-small and C-small, which would have been enough.

**Problem A, Bullseye:** Draw some concentric rings, with a fixed amount of paint.  A bit of maths shows that ring \\( n \\in \\{1,2,3,\cdots\\} \\) uses \\( 2r+4n-3 \\) units of paint, so we want the maximal \\( N \\) with \\[ t \geq \sum_{n=1}^N 2r+4n-3 = 2rN + 2N(N+1) - 3N. \\]

<!--more-->

You can solve this exactly, but with doubles, the needed square-root loses accuracy (they were nice enough to give an example which shows this).  So I ran a couple of interations of Newton-Rapheson using integer arithematic in Python.  An alternative would have been binary search with 64-bit integers.


**Problem B, Manage Your Energy:** Start with `E` energy, then each day decide how much energy to use, say \\(e_i\\) to "gain" \\( e_i v_i \\), then your energy decreases by this amount (and cannot go below 0) but then is recharged by `R`, but is capped at `E`.  So we have something like \\( E\_0 = E, 0 \\leq e\_i \\leq E\_{i-1}, E\_i = \\min(E, E\_{i-1} - e\_i + R) \\) and we want to maximise the gain \\( \sum_{i=1}^N e_iv_i \\).

The small case you can brute-force (in Python, a little optimisation is needed, noting that you can take \\( e_1 \geq R \\) and \\( e\_N = E\_{N-1} \\).)  For the large case, I eventually came up with the following idea (which seems different, though slower, than the offical answer).  For later recursion, suppose we _also_ want to ensure that we finish with at least `F` energy, so \\( E_N \geq F \\), so the initial problem has \\( F=0 \\).  Let \\( v_k \\) be maximal, and imagine trying to increase \\( e_k \\):

   - If \\( e_k = E \\) we can't
   - If \\( e\_k = E\_{k-1} < E \\) then be decreasing \\( e\_{k-1} \\), or if not possible, then \\( e\_{k-2} \\) and so on, we can increase \\( E\_{k-1} \\).  As \\( v\_k \\) is maximal, this cannot decrease the overall "gain".  In this way, we can increase \\( E\_{k-1} \\) to the maximum it can be, which is \\( E\_0 + (k-1)R \\).
   - If \\( e\_k < E\_{k-1} \\) then increasing \\( e\_k \\) will decrease \\( E\_k \\) and so maybe we'll be forced to decrease \\( e\_{k+1} \\) etc.  Again this is okay as far as overall "gain" is concerned.  So the only constraint is to ensure \\( E\_N = e\_{k-1} + (N-k+1)R \geq F \\).

In this way, we can maximise \\( e\_k \\) and then solve the intervals \\( [1,k-1], [k+1,N] \\) recursively.  We must do these in order though, as for \\( [1,k-1] \\) we know the values of \\( E_0, F \\) but for \\( [k+1,N]\\) we don't know the eventual starting energy value yet.  So just push onto a LIFO stack in the correct order.  This gives an \\( O(N^2) \\) algorithm, because of having to find the maximal \\( v\_k \\) for each sub-interval.


**Problem C, Good Luck:** More probability.  Given chosen numbers \\( (A\_i)\_{i=1}^N \\) uniformly at random in \\( \\{ 2,3,\cdots,M \\} \\) and random subsets \\( B\_1,\cdots,B\_K \\) we're told the products \\( p\_i = \prod\_{j\in B\_i} A\_j \\).  Try to guess the \\( A\_i \\).  Well, by Bayes, we're trying to maximise \\[ \mathbb P(A\|p) = \frac{\mathbb P(p\|A) \mathbb P(A)}{\mathbb P(p)} \propto \mathbb P(p\|A) \\] which as ever makes sense: try to choose the \\( A=(A\_i) \\) which make the products most likely to be seen.

For the small case, we can compute all choices, because we're given the constraints.  For the large case, we need a better approach.  This is actually easy once you realise the trick: there are seemingly \\( (M-1)^N = 7^{12} \\) choices for A, but the order of the \\( A\_i \\) doesn't matter!  Let's think about how to count how many unordered choices there are:

   - We need to choose how many 2s, 3s, ..., 8s there are, say \\( 0 \leq x\_2, \cdots, x\_8 \\) with \\( \\sum x\_i = N = 12 \\).
   - I'll use the [Stars and Bars](https://en.wikipedia.org/wiki/Stars_and_bars_%28combinatorics%29) method.
   - Let \\( y_i = x_i+1 \\) so want to find choices for \\( 1\leq y_i \\) with \\( \\sum y\\_i = N+(M-1) \\).
   - Write down \\( N+M-1\\) stars and then puts \\( M-2 \\) bars between the stars, and then read off the \\( y_i \\) the the number of stars in each segment, e.g. `**|*|**|***` gives \\( y\_2=2, y\_3=1, y\_4=2, y\_5=3 \\).
   - There are \\( N+M-2 \\) gaps between the stars so we have \\( N+M-2 \\) choose \\( M-2 \\) choices.
   - For the large case, this is 18 choose 6 or just 18564 choices.
   - _Don't forget_ that now \\( \mathbb P(A) \\) varies, if we consider \\(A\\) as an unordered sequence.  The number of choices is given by [Multinomial coefficients](https://en.wikipedia.org/wiki/Multinomial_theorem#Number_of_unique_permutations_of_words) so in our case, \\( N! / x\_2!\cdots x\_M! \\).

Then we generate all choices for \\(A\\) and then find all possible products and the relative chance of that product occuring (the number of choices for subset \\(B\\) which gives that product).  As each choice of subset (and hence of product) is independent, \\( \mathbb P(p\|A) = \\prod \mathbb P(p\_i\|A) \\).  So we search for which choices of \\(A\\) are consistent will all products, and then select the most likely.

To speed this up, my Python solution uses some dictionaries: one mapping products to possible choices for A and then one mapping pairs (product, A) to the relative chance.  The [IPython notebook](http://nbviewer.ipython.org/github/MatthewDaws/CodeJam/blob/master/2013_1a/Problem%20C.ipynb) shows that out of 8000 goes, even this "perfect" solution can only expect to right around 1300 times.